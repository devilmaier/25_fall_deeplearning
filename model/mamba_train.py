import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import glob
import os
import sys

from mamba_dataloader import CryptoMambaDataset

import causal_conv1d_cuda
from mamba_ssm.modules.mamba_simple import Mamba

BATCH_SIZE = 16        # GPU ë©”ëª¨ë¦¬ì— ë§žì¶° ì¡°ì ˆ (16, 32, 64 ...)
LEARNING_RATE = 1e-4   # Mamba/Transformer ê³„ì—´ì€ ë³´í†µ ë‚®ê²Œ ì‹œìž‘
EPOCHS = 10
SEQ_LEN = 240
N_COINS = 30
N_FEATURES = 258
DATA_DIR = "./data"    # h5 íŒŒì¼ë“¤ì´ ë“¤ì–´ìžˆëŠ” í´ë” ê²½ë¡œ

# GPU ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

class RMSNorm(nn.Module):
    """Mambaì—ì„œ ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” RMS Normalization"""
    def __init__(self, d_model: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(d_model))

    def forward(self, x):
        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return output * self.weight

class CryptoMamba(nn.Module):
    def __init__(self, 
                 d_input=258,     # ìž…ë ¥ Feature ìˆ˜
                 d_model=128,     # ëª¨ë¸ ë‚´ë¶€ Hidden Dimension (ì¡°ì ˆ ê°€ëŠ¥)
                 n_layers=4,      # Mamba ë ˆì´ì–´ ìŒ“ëŠ” íšŸìˆ˜
                 n_coins=30,      # ì½”ì¸ ê°œìˆ˜
                 dropout=0.1):
        super().__init__()
        
        self.d_input = d_input
        self.d_model = d_model
        self.n_coins = n_coins

        # 1. Feature Projection (258 -> 128)
        # ìž…ë ¥ ì°¨ì›ì„ ëª¨ë¸ ì°¨ì›ìœ¼ë¡œ ë³€í™˜
        self.encoder = nn.Linear(d_input, d_model)
        
        # 2. Mamba Backbone
        # Mamba ë¸”ë¡ë“¤ì„ ìŒ“ì•„ ì‹œê³„ì—´ íŠ¹ì„± í•™ìŠµ
        self.layers = nn.ModuleList([
            Mamba(
                d_model=d_model, # Model dimension d_model
                d_state=16,      # SSM state expansion factor
                d_conv=4,        # Local convolution width
                expand=2,        # Block expansion factor
            ) for _ in range(n_layers)
        ])
        
        # ë ˆì´ì–´ ì‚¬ì´ì˜ ì •ê·œí™” (Normalization)
        self.norms = nn.ModuleList([RMSNorm(d_model) for _ in range(n_layers)])
        
        # 3. Final Prediction Head
        # ì‹œí€€ìŠ¤ì˜ ë§ˆì§€ë§‰ ìƒíƒœë¥¼ ì´ìš©í•´ ìˆ˜ìµë¥  ì˜ˆì¸¡
        self.final_norm = RMSNorm(d_model)
        self.head = nn.Sequential(
            nn.Linear(d_model, 64),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(64, 1) # ìµœì¢… ì¶œë ¥: 1ê°œì˜ ìŠ¤ì¹¼ë¼ (ì˜ˆì¸¡ ìˆ˜ìµë¥ )
        )

    def forward(self, x):
        """
        Input x: (Batch, Seq_Len, Coins, Features) -> (B, 240, 30, 258)
        """
        B, L, N, F = x.shape
        
        # [Step 1] Reshape for Mamba
        # MambaëŠ” (Batch, Seq_Len, Dim) ìž…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.
        # ì—¬ê¸°ì„œ ê° ì½”ì¸ì„ ë…ë¦½ì ì¸ ìƒ˜í”Œë¡œ ì·¨ê¸‰í•˜ì—¬ ë°°ì¹˜ ì°¨ì›ê³¼ í•©ì¹©ë‹ˆë‹¤.
        # (Batch * Coins, Seq_Len, Features)
        x = x.view(B * N, L, F)
        
        # [Step 2] Feature Projection
        x = self.encoder(x) # (B*N, L, d_model)
        
        # [Step 3] Mamba Layers (Residual Connection ì ìš©)
        for layer, norm in zip(self.layers, self.norms):
            # Pre-Norm & Residual
            x_norm = norm(x)
            x = x + layer(x_norm)
            
        # [Step 4] Prediction
        # ì‹œí€€ìŠ¤ì˜ ê°€ìž¥ ë§ˆì§€ë§‰ ì‹œì (t)ì˜ ì •ë³´ë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        x = self.final_norm(x)
        last_state = x[:, -1, :] # (B*N, d_model)
        
        out = self.head(last_state) # (B*N, 1)
        
        # [Step 5] Reshape back to (Batch, Coins)
        return out.view(B, N)

def get_dataloaders():
    # 1. íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ë° ì •ë ¬ (ë‚ ì§œ ìˆœì„œ ë³´ìž¥ í•„ìˆ˜)
    file_paths = sorted(glob.glob(os.path.join(DATA_DIR, "*.h5")))
    
    if len(file_paths) == 0:
        raise ValueError(f"{DATA_DIR} í´ë”ì— .h5 íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
    print(f"Total files found: {len(file_paths)}")

    # 2. ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  (Time Series Split)
    # ì˜ˆ: 31ê°œ íŒŒì¼ -> Train(24), Val(3), Test(4)
    n_total = len(file_paths)
    n_train = int(n_total * 0.8) # ì•½ 24ì¼
    n_val = int(n_total * 0.1)   # ì•½ 3ì¼
    
    train_files = file_paths[:n_train]
    val_files = file_paths[n_train : n_train + n_val]
    test_files = file_paths[n_train + n_val :]
    
    print(f"Split: Train({len(train_files)}) / Val({len(val_files)}) / Test({len(test_files)})")

    # 3. Dataset & DataLoader ìƒì„±
    # Trainì€ ì…”í”Œì„ í•´ë„ ë¨ (ìœˆë„ìš° ë‹¨ìœ„ë¡œ ìž˜ë ¤ìžˆìœ¼ë¯€ë¡œ ìˆœì„œ ìƒê´€ ì—†ìŒ, ì˜¤ížˆë ¤ í•™ìŠµì— ë„ì›€)
    train_dataset = CryptoMambaDataset(train_files, seq_len=SEQ_LEN)
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    
    # Val/TestëŠ” ìˆœì„œëŒ€ë¡œ í‰ê°€í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 
    val_dataset = CryptoMambaDataset(val_files, seq_len=SEQ_LEN)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
    
    return train_loader, val_loader

def train_model():
    train_loader, val_loader = get_dataloaders()
    
    # ëª¨ë¸ ì´ˆê¸°í™” (ì‹¤ì œ CryptoMamba ëª¨ë¸ë¡œ êµì²´ í•„ìš”)
    model = CryptoMamba().to(device)
    
    # Loss & Optimizer
    criterion = nn.MSELoss() # íšŒê·€ ë¬¸ì œì´ë¯€ë¡œ MSE ì‚¬ìš©
    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    
    best_val_loss = float('inf')

    print("\nStarting Training...")
    
    for epoch in range(EPOCHS):
        # --- Train ---
        model.train()
        train_loss = 0.0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            # data: (B, 240, 30, 258), target: (B, 30)
            data, target = data.to(device), target.to(device)

            # --- [ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€] ---
            if torch.isnan(data).any():
                print(f"ðŸš¨ [Error] ìž…ë ¥ ë°ì´í„°(X)ì— NaNì´ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤! (Batch {batch_idx})")
                break
            if torch.isinf(data).any():
                print(f"ðŸš¨ [Error] ìž…ë ¥ ë°ì´í„°(X)ì— ë¬´í•œëŒ€(Inf)ê°€ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤! (Batch {batch_idx})")
                break
            if torch.isnan(target).any():
                print(f"ðŸš¨ [Error] íƒ€ê²Ÿ ë°ì´í„°(Y)ì— NaNì´ í¬í•¨ë˜ì–´ ìžˆìŠµë‹ˆë‹¤! (Batch {batch_idx})")
                break
            # ------------------------
            
            optimizer.zero_grad()
            
            # Forward
            output = model(data) # Output: (B, 30)
            
            # Loss ê³„ì‚°
            loss = criterion(output, target)
            
            # Backward
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            if batch_idx % 10 == 0:
                print(f"Epoch [{epoch+1}/{EPOCHS}] Step [{batch_idx}/{len(train_loader)}] Loss: {loss.item():.6f}")

        avg_train_loss = train_loss / len(train_loader)

        # --- Validation ---
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                
                output = model(data)
                loss = criterion(output, target)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        print(f"==> Epoch [{epoch+1}/{EPOCHS}] Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}")
        
        # Best Model ì €ìž¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), "best_cryptomamba_model.pth")
            print("    (Best model saved)")

if __name__ == "__main__":
    train_model()
